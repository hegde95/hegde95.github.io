# section information
section:
  name: Select Publications
  id: publications
  enable: true
  weight: 3
  showOnNavbar: true
  # Can optionally hide the title in sections
  # hideTitle: true

# filter buttons
buttons:
- name: All
  filter: "all"
- name: "Reinfocement Learning"
  filter: "reinforcement learning"
- name: "Diffusion"
  filter: "diffusion"
- name: Robotics
  filter: "robotics"
- name: "Autonomous Vehicles"
  filter: "autonomous vehicles"
- name: "World Models"
  filter: "world models"
- name: "Machine Learning"
  filter: "machine learning"
- name: "Signal Processing"
  filter: "signal processing"
# - name: "QEC"
#   filter: "quantum error correction"
# your publications
publications:
- title: Latent Weight Diffusion, Generating reactive policies instead of trajectories.
  publishedIn:
    name: NeurIPS 2025 Embodied World Models for Decision Making Workshop, RSS 2025 Resource constrained robotics workshop, Submitted to ICLR 2026
    date: 2025
    # url: https://sites.google.com/view/policydiffusion/home
  authors:
  - name: Shashank Hegde
    url: https://hegde95.github.io/
  - name: Satyajeet Das
    url: https://www.linkedin.com/in/satyajeet-das/
  - name: Dr. Gautam Salhotra
    url: https://www.gautamsalhotra.com/  
  - name: Dr. Gaurav Sukhatme
    url: https://robotics.usc.edu/resl/people/1/
  paper:
    summary:  Currently, large generalized policies are trained to predict controls or trajectories using diffusion models, which have the desirable property of learning multimodal action distributions. However, generalizability comes with a cost, namely, larger model size and slower inference. This is especially an issue for robotic tasks that require high control frequency. Further, there is a known trade-off between performance and action horizon for Diffusion Policy (DP), a popular model for generating trajectories - fewer diffusion queries accumulate greater trajectory tracking errors. For these reasons, it is common practice to run these models at high inference frequency, subject to robot computational constraints. To address these limitations, we propose Latent Weight Diffusion (LWD), a method that uses diffusion and a world model to generate closed-loop policies (weights for neural policies) for robotic tasks, rather than generating trajectories. Learning the behavior distribution through parameter space over trajectory space offers two key advantages - longer action horizons (fewer diffusion queries) & robustness to perturbations while retaining high performance; and a lower inference compute cost. To this end, we show that LWD has higher success rates than DP when the action horizon is longer and when stochastic perturbations exist in the environment. Furthermore, LWD achieves multitask performance comparable to DP while requiring just ~ 1/45th of the inference-time FLOPS per step.
    url: files/EWM_workshop.pdf
  categories: ["diffusion","robotics","world models"]
  tags: ["diffusion", "robotics", "world models"]

  ##########################################################################

- title: Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models.
  publishedIn:
    name: ICRA 2025, Robots in the wild workshop, Submitted to ICRA 2026
    date: 2025
    # url: https://sites.google.com/usc.edu/hyperppo
  authors:
  - name: Alexander Popov
    url: https://www.linkedin.com/in/alexander-popov-a1398b69/
  - name: Alperen Degirmenci
    url: https://www.linkedin.com/in/alperen-degirmenci-66404735/
  - name: David Wehr
    url: https://www.linkedin.com/in/davidawehr/
  - name: Shashank Hegde
    url: https://hegde95.github.io/
  - name: Ryan Oldja
    url: https://www.linkedin.com/in/roldja/
  - name: Dr. Alexey Kamenev
    url: https://www.linkedin.com/in/alexeykamenev/
  - name: Dr. Bertrand Douillard
    url: https://www.linkedin.com/in/bertrand-douillard-6804b314/
  - name: David Nister
    url: https://www.linkedin.com/in/david-nister/
  - name: Dr. Urs Muller
    url: https://www.linkedin.com/in/ursmuller/
  - name: Ruchi Bhargava
    url: https://www.linkedin.com/in/ruchibhargava/
  - name: Dr. Stan Birchfield
    url: https://sbirchfield.github.io/
  - name: Dr. Nikolai Smolyanskiy
    url: https://www.linkedin.com/in/nikolai-smolyanskiy-84b068a/
  paper:
    summary:  We propose the use of latent space generative world models to address the covariate shift problem in autonomous driving. A world model is a neural network capable of predicting an agent's next state given past states and actions. By leveraging a world model during training, the driving policy effectively mitigates covariate shift without requiring an excessive amount of training data. During end-to-end training, our policy learns how to recover from errors by aligning with states observed in human demonstrations, so that at runtime it can recover from perturbations outside the training distribution. Additionally, we introduce a novel transformer-based perception encoder that employs multi-view cross-attention and a learned scene query. We present qualitative and quantitative results, demonstrating significant improvements upon prior state of the art in closed-loop testing in the CARLA simulator, as well as showing the ability to handle perturbations in both CARLA and NVIDIA's DRIVE Sim.
    url: https://www.youtube.com/watch?v=7m3bXzlVQvU
  categories: ["world models", "autonomous vehicles"]
  tags: ["world models", "autonomous vehicles"]

  ##########################################################################

- title: Optimizing continuous-time quantum error correction for arbitrary noise
  publishedIn:
    name: Arxiv
    date: 2025
    # url: https://sites.google.com/usc.edu/hyperppo
  authors:
  - name: Anirudh Lanka
    url: https://www.linkedin.com/in/anirudhlanka98/
  - name: Shashank Hegde
    url: https://hegde95.github.io/
  - name: Dr. Todd Andrew Brun
    url: https://viterbi.usc.edu/directory/faculty/Brun/Todd
  paper:
    summary:  We present a protocol using machine learning (ML) to simultaneously optimize the quantum error-correcting code space and the corresponding recovery map in the framework of continuous-time quantum error correction. Given a Hilbert space and a noise process -- potentially correlated across both space and time -- the protocol identifies the optimal recovery strategy, measured by the average logical state fidelity. This approach enables the discovery of recovery schemes tailored to arbitrary device-level noise.
    url: https://arxiv.org/abs/2506.21707
  categories: ["machine learning","quantum error correction"]
  tags: ["machine learning", "quantum error correction"]

  ##########################################################################

- title: Active steering into quantum stabilizer codespace with reinforcement learning
  publishedIn:
    name: APS March Meeting 2024
    date: 2024
    # url: https://sites.google.com/usc.edu/hyperppo
  authors:
  - name: Anirudh Lanka
    url: https://www.linkedin.com/in/anirudhlanka98/
  - name: Dr. Prithviraj Prabhu
    url: https://www.linkedin.com/in/prithviraj-prabhu-91051b108/
  - name: Shashank Hegde
    url: https://hegde95.github.io/
  - name: Dr. Todd Andrew Brun
    url: https://viterbi.usc.edu/directory/faculty/Brun/Todd
  paper:
    summary:  The quantum error correction protocol has been a practical problem in quantum computation, especially in measuring high-weight stabilizers and decoding the error syndrome to find recovery operators. We propose a technique to actively maintain a quantum stabilizer codestate in the codespace even under the influence of decoherence. Our protocol uses continuous measurements of operators from the stabilizer algebra to perform Hamiltonian corrections. The measurement operators and the correction strengths are provided by a reinforcement learning agent. We process the measurement data by first applying an exponential averaging filter and then stacking the previous measurement outcomes before sending them to a reinforcement learning agent. The agent then provides correction strengths and the subsequent measurement operators. We demonstrate that this protocol can evolve any unknown quantum state into a stabilizer code state, and also maintain it within the codespace. This technique is particularly useful since it is scalable to higher dimensional quantum stabilizer codes.
    url: https://meetings.aps.org/Meeting/MAR24/Session/A49.5
  categories: ["reinforcement learning","quantum error correction"]
  tags: ["reinforcement learning", "quantum error correction"]

  ##########################################################################

- title: HyperPPO- A scalable method for finding small policies for robotic control
  publishedIn:
    name: ICRA 2024
    date: 2024
    # url: https://sites.google.com/usc.edu/hyperppo
  authors:
  - name: Shashank Hegde
    url: https://hegde95.github.io/
  - name: Zhehui Huang
    url: https://zhehui-huang.github.io/
  - name: Dr. Gaurav Sukhatme
    url: https://robotics.usc.edu/resl/people/1/
  paper:
    summary:  We propose HyperPPO, an on-policy reinforcement learning algorithm that utilizes graph hypernetworks to estimate the weights of multiple neural architectures simultaneously. Our method estimates weights for networks that are much smaller than those in common-use networks yet encode highly performant policies. We obtain multiple trained policies at the same time while maintaining sample efficiency and provide the user the choice of picking a network architecture that satisfies their computational constraints.
    url: https://sites.google.com/usc.edu/hyperppo
  categories: ["reinforcement learning","robotics"]
  tags: ["reinforcement learning", "robotics"]

  ##########################################################################

- title: Generating Behaviorally Diverse Policies with Latent Diffusion Models
  publishedIn:
    name: NeurIPS 2023
    date: 2023
    # url: https://sites.google.com/view/policydiffusion/home
  authors:
  - name: Shashank Hegde
    url: https://hegde95.github.io/
  - name: Sumeet Batra
    url: https://sumeetbatra.github.io/
  - name: Dr. Gaurav Sukhatme
    url: https://robotics.usc.edu/resl/people/1/
  paper:
    summary:  In this work, we propose using diffusion models to distill a dataset of policies into a single generative model over policy parameters. We show that our method achieves a compression ratio of 13x while recovering 98% of the original rewards and 89% of the original coverage. Furthermore, the conditioning mechanism of diffusion models allows for flexibly selecting and sequencing behaviors using language.
    url: https://sites.google.com/view/policydiffusion/home
  categories: ["diffusion","robotics"]
  tags: ["diffusion", "robotics"]

  ##########################################################################

- title: Efficiently Learning Small Policies for Locomotion and Manipulation
  publishedIn:
    name: ICRA 2023
    date: 2023
    url: https://www.icra2023.org/
  authors:
  - name: Shashank Hegde
    url: https://hegde95.github.io/
  - name: Dr. Gaurav Sukhatme
    url: https://robotics.usc.edu/resl/people/1/

  paper:
    summary:  We leverage graph hyper networks to learn graph hyper policies trained with off-policy reinforcement learning resulting in networks that are two orders of magnitude smaller than commonly used networks yet encode policies comparable to those encoded by much larger networks trained on the same task. 
    url: https://sites.google.com/usc.edu/graphhyperpolicy/home
  categories: ["reinforcement learning","robotics"]
  tags: ["reinforcement learning", "robotics"]

  ##########################################################################

- title: Guided Learning of Robust Hurdling Policies with Curricular Trajectory Optimization
  publishedIn:
    name:  Southern California Robotics Symposium
    date: 2022
    url: https://www.scr.ucla.edu/
  authors:
  - name: Dr. Gautam Salhotra
    url: https://www.gautamsalhotra.com/
  - name: Shashank Hegde
    url: https://hegde95.github.io/
  - name: Sumeet Batra
    url: https://sumeetbatra.github.io/
  - name: Dr. Peter Englert
    url: http://www.peter-englert.net/
  - name: Dr. Gaurav Sukhatme
    url: https://robotics.usc.edu/resl/people/1/

  paper:
    summary: In this work, we focus on the combination of analytical and learning-based techniques to help researchers solve challenging robot locomotion problems. Specifically, we explore the combination of curricular trajectory optimization (CTO) and deep reinforcement learning (RL) for quadruped hurdling tasks.
    url: https://sites.google.com/usc.edu/cto-rl/home
  categories: ["reinforcement learning", "robotics"]
  tags: ["reinforcement learning", "robotics"]

  ##########################################################################

- title: Agents that Listen, High-Throughput Reinforcement Learning with Multiple Sensory Systems
  publishedIn:
    name:  IEEE Conference on Games
    date: 2021
    url: https://ieee-cog.org/2021/
  authors:
  - name: Shashank Hegde
    url: https://hegde95.github.io/
  - name: Dr. Anssi Kanervisto
    url: https://www.microsoft.com/en-us/research/people/t-anssik/
  - name: Dr. Aleksie Petrenko
    url: https://alex-petrenko.github.io/
  paper:
    summary: We introduce a new version of VizDoom simulator to create a highly efficient learning environment that provides raw audio observations. We study the performance of different model architectures in a series of tasks that require the agent to recognize sounds and execute instructions given in natural language. Finally, we train our agent to play the full game of Doom and find that it can consistently defeat a traditional vision-based adversary.
    url: https://sites.google.com/view/sound-rl
  categories: ["reinforcement learning", "signal processing"]
  tags: ["reinforcement learning", "signal processing"]

  ##########################################################################

- title: Randomized Policy Learning for Continuous State and Action MDPs
  publishedIn:
    name:  Arxiv
    date: 2020
    # url: https://ieee-cog.org/2021/
  authors:
  - name: Dr. Hiteshi Sharma
    url: https://www.microsoft.com/en-us/research/people/hitshar/
  - name: Shashank Hegde
    url: https://hegde95.github.io/
  - name: Dr. Rahul Jain
    url: https://www.rahuljain.net/
  paper:
    summary:  We present RANDPOL, a generalized policy iteration algorithm for MDPs with continuous state and action spaces. Both the policy and value functions are represented with randomized networks. We also give finite time guarantees on the performance of the algorithm.
    url: https://arxiv.org/pdf/2006.04331.pdf
  categories: ["reinforcement learning"]
  tags: ["reinforcement learning"]

  ##########################################################################

- title: Risk aware portfolio construction using deep deterministic policy gradients
  publishedIn:
    name:  IEEE Symposium Series on Computational Intelligence (SSCI)
    date: 2018
    # url: https://ieee-cog.org/2021/
  authors:
  - name: Shashank Hegde
    url: https://hegde95.github.io/
  - name: Vishal Kumar
    # url: https://www.rahuljain.net/
  - name: Dr. Atul Singh
    # url: https://www.microsoft.com/en-us/research/people/asingh/
  paper:
    summary:  This paper evaluates the use of DDPG to solve the problem of risk aware portfolio construction. Simulations are done on a portfolio of twenty stocks and the use of both Rate of Return and Sortino ratio as a measure of portfolio performance are evaluated. Results are presented that demonstrate the effectiveness of DDPG for risk aware portfolio construction.
    url: https://ieeexplore.ieee.org/abstract/document/8628791
  categories: ["reinforcement learning", "finance"]
  tags: ["reinforcement learning", "finance"]

- title: Use of light emitting diodes (LEDs) for enhanced lipid production in micro-algae based biofuels
  publishedIn:
    name: Journal of Photochemistry and Photobiology B, Biology
    date: 2017
    # url: https://ieee-cog.org/2021/
    # Alifha Severes a,Shashank Hegde b, L. D'Souza a, Smitha Hegde
  authors:
  - name: Dr. Alifha Severes
    # url: https://www.rahuljain.net/
  - name: Shashank Hegde
    url: https://hegde95.github.io/
  - name: Dr. L. D'Souza
    # url: https://www.rahuljain.net/
  - name: Dr. Smitha Hegde
    # url: https://www.rahuljain.net/
  paper:
    summary:  Microalgae are an alternative source for renewable energy to overcome the energy crises caused by exhaustion of fuel reserves. Algal biofuel technology demands a cost effective strategy for net profitable productivity. Inconsistent illumination intensities hinder microalgal growth. The light-utilizing efficiency of the cells is critical. Light scarcity leads to low production and high intensities cause photo-inhibition. We report effective usage of LEDs of different band wavelengths on the growth of microalgae in a closed, controlled environment to generate biomass and lipid yields. Among the different intensity and wavelengths tested. The light intensities of 500 lx of blue-red combination gave maximum biomass in terms of cell density. LED of red light 220 lx wavelength doubled the lipid dry weight from 30% (w/w) in white light to 60% (w/w). Thin layer lipid chromatogram demonstrated a dense and prominent spot of triacylglycerols in the red light, 220 lx grown cultures. The FTIR profile indicates that different wavelength exposure did not alter the functional groups or change the chemical composition of the extracted lipids ensuring the quality of the product. We reiterate the fact that combination of red and blue LEDs is favoured over white light illumination for generation of biomass. In addition, we report an exciting finding of exposure to LEDs of red wavelength post-biomass generation lead to enhanced lipid production. This simple process doubled the lipid content harvested in 20 days culture period.
    url: https://www.sciencedirect.com/science/article/abs/pii/S1011134416308120
  categories: []
  tags: []